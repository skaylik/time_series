"""
–ú–æ–¥—É–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–≠—Ç–∞–ø 2).
–°–æ–∑–¥–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ª–∞–≥–∏, —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –∏ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏),
–≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train/validation/test –≤—ã–±–æ—Ä–∫–∏.
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st

from utils import parse_int_list


def engineer_time_features(
    transformed_series: pd.Series,
    datetime_series: pd.Series,
    lags: List[int],
    rolling_windows: List[int],
    exogenous: Optional[pd.DataFrame] = None,
) -> pd.DataFrame:
    target_name = transformed_series.name or "target"
    datetime_name = datetime_series.name or "datetime"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if transformed_series.empty:
        raise ValueError("transformed_series –ø—É—Å—Ç–æ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∞–ø–µ 1.")
    if datetime_series.empty:
        raise ValueError("datetime_series –ø—É—Å—Ç–æ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∞–ø–µ 1.")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –Ω–µ-NaN –∑–Ω–∞—á–µ–Ω–∏–µ
    if transformed_series.notna().sum() == 0:
        raise ValueError("transformed_series —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ NaN. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∞–ø–µ 1.")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º datetime_series –≤ datetime, –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω
    datetime_converted = pd.to_datetime(datetime_series, errors="coerce")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–ª–∏–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
    if len(transformed_series) != len(datetime_series):
        raise ValueError(
            f"–î–ª–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ ({len(transformed_series)}) –∏ –¥–∞—Ç ({len(datetime_series)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç. "
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∞–ø–µ 1."
        )
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DataFrame
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–±–∞ Series –ø–æ –∏–Ω–¥–µ–∫—Å—É transformed_series –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º DataFrame
    if isinstance(transformed_series.index, pd.DatetimeIndex):
        # –ï—Å–ª–∏ transformed_series –∏–º–µ–µ—Ç DatetimeIndex, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –¥–∞—Ç—É
        datetime_for_df = pd.Series(transformed_series.index, index=transformed_series.index, name=datetime_name)
    else:
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º datetime_converted –ø–æ –∏–Ω–¥–µ–∫—Å—É transformed_series
        if transformed_series.index.equals(datetime_converted.index):
            datetime_for_df = datetime_converted
        else:
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É
            datetime_for_df = datetime_converted.reindex(transformed_series.index)
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –µ—Å—Ç—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º –∏—Ö –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
            if datetime_for_df.isna().any():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ .values
                datetime_for_df = pd.Series(
                    datetime_converted.values, 
                    index=transformed_series.index, 
                    name=datetime_name
                )
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã—Ö Series
    combined = pd.DataFrame({
        target_name: transformed_series,
        datetime_name: datetime_for_df
    })
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    if combined.empty:
        raise ValueError(
            f"–ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è selected_series –∏ datetime_series –ø–æ–ª—É—á–∏–ª—Å—è –ø—É—Å—Ç–æ–π DataFrame. "
            f"selected_series –¥–ª–∏–Ω–∞={len(transformed_series)}, datetime_series –¥–ª–∏–Ω–∞={len(datetime_series)}, "
            f"selected_series –∏–Ω–¥–µ–∫—Å —Ç–∏–ø={type(transformed_series.index)}, "
            f"datetime_series –∏–Ω–¥–µ–∫—Å —Ç–∏–ø={type(datetime_series.index)}. "
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç –∏–ª–∏ –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É."
        )

    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –û–ë–ê (–∏ –¥–∞—Ç–∞, –∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è) NaN –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    # –≠—Ç–æ –≤–∞–∂–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –ª–∞–≥–∏ —Å–æ–∑–¥–∞—é—Ç NaN –≤ –Ω–∞—á–∞–ª–µ, –Ω–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å
    initial_len = len(combined)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
    if initial_len == 0:
        raise ValueError(
            f"DataFrame –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è. "
            f"transformed_series –¥–ª–∏–Ω–∞={len(transformed_series)}, datetime_series –¥–ª–∏–Ω–∞={len(datetime_series)}."
        )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    if target_name not in combined.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ {target_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ combined. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {combined.columns.tolist()}")
    if datetime_name not in combined.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ {datetime_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ combined. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {combined.columns.tolist()}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ-NaN –∑–Ω–∞—á–µ–Ω–∏–π
    target_notna = combined[target_name].notna().sum()
    datetime_notna = combined[datetime_name].notna().sum()
    
    # –ï—Å–ª–∏ –¥–∞—Ç–∞ NaN, –Ω–æ –∏–Ω–¥–µ–∫—Å - —ç—Ç–æ DatetimeIndex, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ –¥–∞—Ç—É
    # –≠—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –î–û —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
    if isinstance(combined.index, pd.DatetimeIndex):
        # –ï—Å–ª–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ –¥–∞—Ç—ã –µ—Å—Ç—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º –∏—Ö –∏–Ω–¥–µ–∫—Å–æ–º
        if combined[datetime_name].isna().any():
            combined[datetime_name] = combined.index
            datetime_notna = len(combined)
        # –ï—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è NaN, –∑–∞–º–µ–Ω—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
        elif combined[datetime_name].isna().all():
            combined[datetime_name] = combined.index
            datetime_notna = len(combined)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ dropna
    original_datetime_values = combined[datetime_name].copy()
    original_index = combined.index.copy()
    original_index_is_datetime = isinstance(combined.index, pd.DatetimeIndex)
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (—ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ)
    before_drop = len(combined)
    combined.dropna(subset=[target_name], inplace=True)
    after_drop_target = len(combined)
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –∑–Ω–∞—á–∏—Ç –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –±—ã–ª–∏ NaN
    if combined.empty:
        raise ValueError(
            f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. "
            f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {initial_len}, –Ω–µ-NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ {target_name}: {target_notna}, "
            f"–Ω–µ-NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ {datetime_name}: {datetime_notna}. "
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ selected_series —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ-NaN –∑–Ω–∞—á–µ–Ω–∏—è."
        )
    
    # –ü–æ—Å–ª–µ dropna –Ω—É–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–∞—Ç—ã –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç—Ä–æ–∫
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ: –Ω–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏, –≥–¥–µ target –Ω–µ NaN
    datetime_after_drop = combined[datetime_name].notna().sum()
    
    # –ï—Å–ª–∏ –¥–∞—Ç–∞ NaN –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö, –∑–∞–ø–æ–ª–Ω—è–µ–º –∏—Ö –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
    if combined[datetime_name].isna().any():
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º transformed_series, –≥–¥–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ NaN (–∏—Å–ø–æ–ª—å–∑—É–µ–º .values –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞)
        valid_positions_mask = transformed_series.notna().values
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ .values –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏
        # –ë–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ datetime_converted –ø–æ –ø–æ–∑–∏—Ü–∏—è–º, –≥–¥–µ target –Ω–µ NaN
        datetime_values_array = datetime_converted.values
        valid_datetime_values = datetime_values_array[valid_positions_mask]
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if len(valid_datetime_values) >= len(combined):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ len(combined) –∑–Ω–∞—á–µ–Ω–∏–π
            combined[datetime_name] = pd.Series(
                valid_datetime_values[:len(combined)],
                index=combined.index,
                name=datetime_name
            )
        else:
            # –ú–µ–Ω—å—à–µ –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π - –∑–∞–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏, –≥–¥–µ –¥–∞—Ç–∞ NaN –≤ combined
            nan_mask = combined[datetime_name].isna()
            nan_count = nan_mask.sum()
            
            if len(valid_datetime_values) >= nan_count:
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ valid_datetime_values
                combined.loc[nan_mask, datetime_name] = valid_datetime_values[:nan_count]
            elif original_index_is_datetime:
                # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å –±—ã–ª DatetimeIndex, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è NaN
                if combined.index.equals(original_index):
                    # –ò–Ω–¥–µ–∫—Å—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è NaN
                    combined.loc[nan_mask, datetime_name] = original_datetime_values[nan_mask]
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ –¥–∞—Ç—É, –µ—Å–ª–∏ —ç—Ç–æ DatetimeIndex
                    if isinstance(combined.index, pd.DatetimeIndex):
                        combined.loc[nan_mask, datetime_name] = combined.index[nan_mask]
                    else:
                        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –∏–∑ valid_datetime_values, –µ—Å–ª–∏ –µ—Å—Ç—å
                        if len(valid_datetime_values) > 0 and nan_count > 0:
                            fill_count = min(nan_count, len(valid_datetime_values))
                            combined.loc[nan_mask, datetime_name] = valid_datetime_values[:fill_count]
                        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å NaN, —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –¥–∞—Ç—ã
                        if combined[datetime_name].isna().any():
                            before_final_drop = len(combined)
                            combined.dropna(subset=[datetime_name], inplace=True)
                            if combined.empty:
                                raise ValueError(
                                    f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –±–µ–∑ –¥–∞—Ç—ã –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. "
                                    f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {initial_len}, –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {after_drop_target}, "
                                    f"–ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º: {before_final_drop}. "
                                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–æ–≤–Ω—è—Ç—å datetime_series."
                                )
            else:
                # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –∏–∑ valid_datetime_values, –µ—Å–ª–∏ –µ—Å—Ç—å
                if len(valid_datetime_values) > 0 and nan_count > 0:
                    fill_count = min(nan_count, len(valid_datetime_values))
                    combined.loc[nan_mask, datetime_name] = valid_datetime_values[:fill_count]
                
                # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å NaN, —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –¥–∞—Ç—ã
                if combined[datetime_name].isna().any():
                    before_final_drop = len(combined)
                    combined.dropna(subset=[datetime_name], inplace=True)
                    if combined.empty:
                        raise ValueError(
                            f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –±–µ–∑ –¥–∞—Ç—ã –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. "
                            f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {initial_len}, –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {after_drop_target}, "
                            f"–ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º: {before_final_drop}. "
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–æ–≤–Ω—è—Ç—å datetime_series."
                        )
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ –¥–∞—Ç–∞ –≤—Å–µ –µ—â–µ NaN, –ø—ã—Ç–∞–µ–º—Å—è —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –¥–∞—Ç—ã
    datetime_final = combined[datetime_name].notna().sum()
    if combined[datetime_name].isna().any():
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞: —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –¥–∞—Ç—ã
        before_final_drop = len(combined)
        combined.dropna(subset=[datetime_name], inplace=True)
        if combined.empty:
            index_type = type(combined.index).__name__
            is_datetime_index = isinstance(combined.index, pd.DatetimeIndex)
            raise ValueError(
                f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –±–µ–∑ –¥–∞—Ç—ã –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. "
                f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {initial_len}, –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {after_drop_target}, "
                f"–Ω–µ-NaN –¥–∞—Ç –¥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {datetime_after_drop}, –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {datetime_final}, "
                f"–ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º: {before_final_drop}, —Ç–µ–∫—É—â–∞—è –¥–ª–∏–Ω–∞: {len(combined)}, "
                f"—Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞: {index_type}, —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex: {is_datetime_index}. "
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ."
            )
    
    if combined.empty:
        raise ValueError(
            f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. "
            f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {initial_len}, –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {after_drop_target}. "
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ selected_series –∏ datetime_series –∏–º–µ—é—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ NaN."
        )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –º–µ–∂–¥—É –∏–º–µ–Ω–µ–º –∫–æ–ª–æ–Ω–∫–∏ –∏ –∏–º–µ–Ω–µ–º –∏–Ω–¥–µ–∫—Å–∞
    # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç –∏–º—è, –∫–æ—Ç–æ—Ä–æ–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å datetime_name, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–º—è –∏–Ω–¥–µ–∫—Å–∞
    if combined.index.name == datetime_name:
        combined.index.name = None
    
    combined.sort_values(datetime_name, inplace=True)

    target_series = combined[target_name]
    for lag in lags:
        combined[f"lag_{lag}"] = target_series.shift(lag)

    for window in rolling_windows:
        rolling = target_series.rolling(window=window, min_periods=1)  # min_periods=1 —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –¥–∞–Ω–Ω—ã–µ
        combined[f"roll_mean_{window}"] = rolling.mean()
        combined[f"roll_std_{window}"] = rolling.std()
        combined[f"roll_min_{window}"] = rolling.min()
        combined[f"roll_max_{window}"] = rolling.max()

    dt = combined[datetime_name]
    combined["dayofweek"] = dt.dt.dayofweek
    combined["month"] = dt.dt.month
    combined["is_holiday"] = dt.dt.dayofweek.isin([5, 6]).astype(int)

    t_week = dt.dt.dayofweek
    t_month = dt.dt.month - 1
    combined["sin_2pi_t_over_7"] = np.sin(2 * np.pi * t_week / 7)
    combined["cos_2pi_t_over_12"] = np.cos(2 * np.pi * t_month / 12)

    if exogenous is not None and not exogenous.empty:
        exog_aligned = exogenous.reindex(combined.index).ffill().bfill()
        combined = combined.join(exog_aligned, how="left")

    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
    # NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–ª–∞–≥–∞—Ö, —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–Ω–∞—Ö) –æ—Å—Ç–∞–≤–ª—è–µ–º - –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
    # –∏–ª–∏ –æ–Ω–∏ –±—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –ø–æ–∑–∂–µ
    initial_len = len(combined)
    combined.dropna(subset=[target_name], inplace=True)
    
    if combined.empty:
        raise ValueError(
            f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. "
            f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {initial_len}, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ."
        )
    
    combined.reset_index(drop=True, inplace=True)
    combined.rename(columns={datetime_name: "datetime"}, inplace=True)
    return combined


def chronological_split(
    features_df: pd.DataFrame,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    n = len(features_df)
    if n == 0:
        return features_df, features_df, features_df

    train_end = int(np.floor(n * train_ratio))
    val_end = train_end + int(np.floor(n * val_ratio))

    train_df = features_df.iloc[:train_end].copy()
    val_df = features_df.iloc[train_end:val_end].copy()
    test_df = features_df.iloc[val_end:].copy()

    return train_df, val_df, test_df


def stage2(
    analysis_data: Optional[Dict[str, Any]],
    lab_state: Dict[str, bool],
    default_lags: List[int],
    default_rolling_windows: List[int],
    default_split: Tuple[int, int, int],
) -> Dict[str, Any]:
    if analysis_data is None:
        analysis_data = {}


    if not lab_state.get("stage1_completed"):
        st.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≤–µ—Ä—à–∏—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –±–ª–æ–∫, —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∫ feature engineering.")
        return analysis_data

    source_df = analysis_data.get("source_df")
    selected_series: Optional[pd.Series] = analysis_data.get("selected_series")
    datetime_series: Optional[pd.Series] = analysis_data.get("datetime_series")

    if source_df is None or selected_series is None or selected_series.empty or datetime_series is None:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –Ω–∞ —ç—Ç–∞–ø–µ 1.")
        return analysis_data

    lag_defaults = analysis_data.get("lag_values", default_lags)
    rolling_defaults = analysis_data.get("rolling_values", default_rolling_windows)
    split_defaults = analysis_data.get("split_percentages", default_split)
    if isinstance(split_defaults, tuple):
        split_defaults = list(split_defaults)
    if not isinstance(split_defaults, (list, tuple)) or sum(split_defaults) != 100:
        split_defaults = list(default_split)
    exog_defaults = analysis_data.get("exog_selection", [])
    selected_pipeline_label = analysis_data.get("selected_pipeline_label", "‚Äî")

    with st.form("feature_engineering_form"):
        feature_col1, feature_col2 = st.columns(2)
        with feature_col1:
            lag_input = st.text_input(
                "–õ–∞–≥–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
                value=", ".join(str(lag) for lag in lag_defaults) if lag_defaults else "",
                help="–í—ã–±–µ—Ä–∏—Ç–µ, –∫–∞–∫–∏–µ –ª–∞–≥–∏ —Å—Ç—Ä–æ–∏—Ç—å. –ü—Ä–∏–º–µ—Ä: 1, 2, 7, 30",
            )
            rolling_input = st.text_input(
                "–û–∫–Ω–∞ –¥–ª—è —Å–∫–æ–ª—å–∑—è—â–∏—Ö (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
                value=", ".join(str(window) for window in rolling_defaults) if rolling_defaults else "",
                help="–†–∞–∑–º–µ—Ä—ã –æ–∫–æ–Ω (–≤ —à–∞–≥–∞—Ö) –¥–ª—è —Å–∫–æ–ª—å–∑—è—â–∏—Ö mean/std/min/max.",
            )
        with feature_col2:
            split_col1, split_col2, split_col3 = st.columns(3)
            train_ratio_input = split_col1.number_input(
                "Train %",
                min_value=10,
                max_value=90,
                value=int(split_defaults[0]),
                step=5,
            )
            val_ratio_input = split_col2.number_input(
                "Validation %",
                min_value=5,
                max_value=80,
                value=int(split_defaults[1]),
                step=5,
            )
            test_ratio_input = split_col3.number_input(
                "Test %",
                min_value=5,
                max_value=80,
                value=int(split_defaults[2]),
                step=5,
            )

        available_exog_columns = [
            col
            for col in source_df.columns
            if col not in {analysis_data["target_column"], analysis_data["date_column"]}
            and pd.api.types.is_numeric_dtype(source_df[col])
        ]
        exog_selection = st.multiselect(
            "–≠–∫–∑–æ–≥–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            available_exog_columns,
            default=exog_defaults,
            help="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ SARIMAX/Prophet.",
        )

        feature_submit = st.form_submit_button("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏")

    if feature_submit:
        has_error = False
        try:
            lag_values = parse_int_list(lag_input) or default_lags
        except ValueError as exc:
            st.error(f"–û—à–∏–±–∫–∞ –≤ —Å–ø–∏—Å–∫–µ –ª–∞–≥–æ–≤: {exc}")
            lag_values = lag_defaults
            has_error = True

        try:
            rolling_values = parse_int_list(rolling_input) or default_rolling_windows
        except ValueError as exc:
            st.error(f"–û—à–∏–±–∫–∞ –≤ —Å–ø–∏—Å–∫–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–æ–Ω: {exc}")
            rolling_values = rolling_defaults
            has_error = True

        ratio_sum = int(train_ratio_input) + int(val_ratio_input) + int(test_ratio_input)
        if ratio_sum != 100:
            st.error("–°—É–º–º–∞ –¥–æ–ª–µ–π Train/Validation/Test –¥–æ–ª–∂–Ω–∞ —Ä–∞–≤–Ω—è—Ç—å—Å—è 100%.")
            has_error = True

        if not has_error:
            train_ratio = train_ratio_input / 100.0
            val_ratio = val_ratio_input / 100.0
            exogenous_df = source_df[exog_selection] if exog_selection else None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_df = None
            train_df = None
            val_df = None
            test_df = None
            
            if selected_series is None or selected_series.empty:
                st.error("‚ùå –û—à–∏–±–∫–∞: –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –ø—É—Å—Ç–æ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —ç—Ç–∞–ø 1.")
                has_error = True
            elif datetime_series is None or datetime_series.empty:
                st.error("‚ùå –û—à–∏–±–∫–∞: —Ä—è–¥ —Å –¥–∞—Ç–∞–º–∏ –ø—É—Å—Ç–æ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —ç—Ç–∞–ø 1.")
                has_error = True
            else:
                try:
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    st.info(f"üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: selected_series –¥–ª–∏–Ω–∞={len(selected_series)}, datetime_series –¥–ª–∏–Ω–∞={len(datetime_series)}")
                    if isinstance(selected_series.index, pd.DatetimeIndex):
                        st.info(f"üîç selected_series –∏–º–µ–µ—Ç DatetimeIndex: {type(selected_series.index)}")
                    if isinstance(datetime_series.index, pd.DatetimeIndex):
                        st.info(f"üîç datetime_series –∏–º–µ–µ—Ç DatetimeIndex: {type(datetime_series.index)}")
                    
                    features_df = engineer_time_features(
                        transformed_series=selected_series,
                        datetime_series=datetime_series,
                        lags=lag_values,
                        rolling_windows=rolling_values,
                        exogenous=exogenous_df,
                    )
                    
                    if features_df.empty:
                        st.error("‚ùå –û—à–∏–±–∫–∞: –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ DataFrame –ø—É—Å—Ç–æ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
                        has_error = True
                    else:
                        train_df, val_df, test_df = chronological_split(
                            features_df,
                            train_ratio=train_ratio,
                            val_ratio=val_ratio,
                        )
                except Exception as exc:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {exc}")
                    st.exception(exc)
                    has_error = True

            if not has_error and features_df is not None:
                lab_state["stage2_completed"] = True
                lab_state["stage3_completed"] = False
                lab_state["stage4_completed"] = False
                lab_state["stage5_completed"] = False

                analysis_data.update(
                    {
                        "features_df": features_df,
                        "train_df": train_df,
                        "val_df": val_df,
                        "test_df": test_df,
                        "lag_values": lag_values,
                        "rolling_values": rolling_values,
                        "split_percentages": [train_ratio_input, val_ratio_input, test_ratio_input],
                        "exog_selection": exog_selection,
                        "target_feature_name": selected_series.name or analysis_data.get("target_column"),
                        "feature_cols": [
                            col for col in features_df.columns if col not in {"datetime", selected_series.name}
                        ],
                        "selected_pipeline_label": selected_pipeline_label,
                    }
                )

                st.success("–ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∏ –≤—ã–±–æ—Ä–∫–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã.")
        else:
            st.warning("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")

    if lab_state.get("stage2_completed"):
        train_df = analysis_data.get("train_df")
        val_df = analysis_data.get("val_df")
        test_df = analysis_data.get("test_df")
        features_df = analysis_data.get("features_df")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
        if train_df is None or val_df is None or test_df is None or features_df is None:
            st.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –≤—ã–±–æ—Ä–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É '–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–±–æ—Ä–æ–∫.")
        else:
            train_len = len(train_df) if hasattr(train_df, '__len__') else 0
            val_len = len(val_df) if hasattr(val_df, '__len__') else 0
            test_len = len(test_df) if hasattr(test_df, '__len__') else 0
            
            if train_len == 0 and val_len == 0 and test_len == 0:
                st.error("‚ùå –í—Å–µ –≤—ã–±–æ—Ä–∫–∏ –ø—É—Å—Ç—ã–µ. –í–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ (dropna) –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–∞–≥–æ–≤.")
            else:
                st.markdown("#### üì¶ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö")
                col1, col2, col3 = st.columns(3)
                col1.metric("Train", train_len)
                col2.metric("Validation", val_len)
                col3.metric("Test", test_len)

        st.markdown("#### üßÆ –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        sample_df = analysis_data.get("features_df", pd.DataFrame())
        if sample_df is not None and not sample_df.empty:
            st.dataframe(sample_df.head(10))
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –Ω–∞–∂–∞–≤ –∫–Ω–æ–ø–∫—É '–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏'.")

        with st.expander("‚ÑπÔ∏è –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**üî¢ –õ–∞–≥–∏:**")
                lag_values = analysis_data.get("lag_values", [])
                if lag_values:
                    st.markdown(f"`{', '.join(map(str, lag_values))}`")
                else:
                    st.info("–ù–µ —É–∫–∞–∑–∞–Ω—ã")
                
                st.markdown("**üìä –°–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞:**")
                rolling_values = analysis_data.get("rolling_values", [])
                if rolling_values:
                    st.markdown(f"`{', '.join(map(str, rolling_values))}`")
                else:
                    st.info("–ù–µ —É–∫–∞–∑–∞–Ω—ã")
                
                st.markdown("**üîÄ –ü—Ä–æ–ø–æ—Ä—Ü–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è:**")
                split_percentages = analysis_data.get("split_percentages", [])
                if split_percentages and len(split_percentages) == 3:
                    split_col1, split_col2, split_col3 = st.columns(3)
                    split_col1.metric("Train", f"{split_percentages[0]}%")
                    split_col2.metric("Val", f"{split_percentages[1]}%")
                    split_col3.metric("Test", f"{split_percentages[2]}%")
                else:
                    st.info("–ù–µ —É–∫–∞–∑–∞–Ω—ã")
            
            with col2:
                st.markdown("**üåê –≠–∫–∑–æ–≥–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:**")
                exog_selection = analysis_data.get("exog_selection", [])
                if exog_selection:
                    for exog in exog_selection:
                        st.markdown(f"- `{exog}`")
                else:
                    st.info("–ù–µ –≤—ã–±—Ä–∞–Ω—ã")
                
                st.markdown("**üîß –í–∞—Ä–∏–∞–Ω—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞:**")
                pipeline_label = analysis_data.get("selected_pipeline_label", "‚Äî")
                st.markdown(f"`{pipeline_label}`")

    return analysis_data


__all__ = ["stage2", "engineer_time_features", "chronological_split"]

