import pandas as pd
import numpy as np
from datetime import datetime
import pytz

# ========== –í–°–ï –§–£–ù–ö–¶–ò–ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò ==========

def standardize_timezone(df, date_column, target_timezone='Europe/Moscow'):
    df_clean = df.copy()
    
    if not pd.api.types.is_datetime64_any_dtype(df_clean[date_column]):
        df_clean[date_column] = pd.to_datetime(df_clean[date_column], errors='coerce')
    
    original_count = len(df_clean)
    df_clean = df_clean.dropna(subset=[date_column])
    invalid_dates_removed = original_count - len(df_clean)
    
    target_tz = pytz.timezone(target_timezone)
    
    if df_clean[date_column].dt.tz is not None:
        df_clean[date_column] = df_clean[date_column].dt.tz_convert(target_tz)
        was_timezone_aware = True
    else:
        df_clean[date_column] = df_clean[date_column].dt.tz_localize(target_tz, ambiguous='NaT', nonexistent='NaT')
        df_clean = df_clean.dropna(subset=[date_column])
        was_timezone_aware = False
    
    standardization_report = {
        'original_records_count': original_count,
        'final_records_count': len(df_clean),
        'invalid_dates_removed': invalid_dates_removed,
        'target_timezone': target_timezone,
        'input_was_timezone_aware': was_timezone_aware
    }
    
    return df_clean, standardization_report

def remove_duplicate_records(df, date_column, strategy='first'):
    df_clean = df.copy()
    original_records_count = len(df_clean)
    
    if strategy in ['first', 'last']:
        df_clean = df_clean.drop_duplicates(subset=[date_column], keep=strategy)
    
    elif strategy == 'mean':
        numeric_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()
        
        if numeric_columns:
            aggregation_dict = {col: 'mean' for col in numeric_columns}
            df_clean = df_clean.groupby(date_column, as_index=False).agg(aggregation_dict)
        else:
            df_clean = df_clean.drop_duplicates(subset=[date_column], keep='first')
    
    duplicates_removed_count = original_records_count - len(df_clean)
    
    deduplication_report = {
        'original_records_count': original_records_count,
        'final_records_count': len(df_clean),
        'duplicates_removed_count': duplicates_removed_count,
        'deduplication_strategy': strategy
    }
    
    return df_clean, deduplication_report

def check_timestamp_monotonicity(df, date_column):
    df_sorted = df.copy().sort_values(by=date_column).reset_index(drop=True)
    
    timestamps = df_sorted[date_column]
    time_differences = timestamps.diff()
    
    backward_time_jumps_count = (time_differences < pd.Timedelta(0)).sum()
    
    if len(time_differences) > 1:
        clean_time_differences = time_differences.dropna()
        if len(clean_time_differences) > 0:
            median_time_interval = clean_time_differences.median()
            std_time_interval = clean_time_differences.std()
            min_time_interval = clean_time_differences.min()
            max_time_interval = clean_time_differences.max()
        else:
            median_time_interval = std_time_interval = min_time_interval = max_time_interval = pd.Timedelta(0)
    else:
        median_time_interval = std_time_interval = min_time_interval = max_time_interval = pd.Timedelta(0)
    
    is_timestamp_sequence_monotonic = df[date_column].is_monotonic_increasing
    
    monotonicity_report = {
        'is_timestamp_sequence_monotonic': is_timestamp_sequence_monotonic,
        'backward_time_jumps_count': backward_time_jumps_count,
        'median_time_interval': str(median_time_interval),
        'std_time_interval': str(std_time_interval),
        'min_time_interval': str(min_time_interval),
        'max_time_interval': str(max_time_interval),
        'total_data_points_count': len(df_sorted)
    }
    
    return monotonicity_report, df_sorted

def handle_missing_exchange_rates(df, date_column, value_column, method='linear', window_size=None):
    df_clean = df.copy()
    
    original_records_count = len(df_clean)
    missing_values_count = df_clean[value_column].isnull().sum()
    missing_values_percentage = (missing_values_count / original_records_count) * 100
    
    if missing_values_count == 0:
        missing_values_report = {
            'original_records_count': original_records_count,
            'missing_values_count': 0,
            'missing_values_percentage': 0.0,
            'imputation_method': 'none',
            'filled_values_count': 0
        }
        return df_clean, missing_values_report
    
    df_indexed_by_time = df_clean.set_index(date_column)
    
    if method == 'drop':
        if missing_values_percentage < 5:
            df_clean = df_clean.dropna(subset=[value_column])
            filled_values_count = 0
        else:
            filled_values_count = 0
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {missing_values_percentage:.2f}% –ø—Ä–æ–ø—É—Å–∫–æ–≤ - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
    
    elif method == 'linear':
        df_clean[value_column] = df_indexed_by_time[value_column].interpolate(method='linear')
        filled_values_count = missing_values_count
    
    elif method == 'polynomial':
        df_clean[value_column] = df_indexed_by_time[value_column].interpolate(method='polynomial', order=2)
        filled_values_count = missing_values_count
    
    elif method == 'cubic':
        df_clean[value_column] = df_indexed_by_time[value_column].interpolate(method='cubic')
        filled_values_count = missing_values_count
    
    elif method == 'rolling_mean':
        if window_size is None:
            window_size = 3
        
        rolling_mean_values = df_clean[value_column].rolling(window=window_size, center=True, min_periods=1).mean()
        df_clean[value_column] = df_clean[value_column].fillna(rolling_mean_values)
        filled_values_count = missing_values_count
    
    elif method == 'forward_fill':
        df_clean[value_column] = df_clean[value_column].ffill()
        filled_values_count = missing_values_count
    
    elif method == 'backward_fill':
        df_clean[value_column] = df_clean[value_column].bfill()
        filled_values_count = missing_values_count
    
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤: {method}")
    
    remaining_missing_values_count = df_clean[value_column].isnull().sum()
    
    missing_values_report = {
        'original_records_count': original_records_count,
        'missing_values_count': missing_values_count,
        'missing_values_percentage': missing_values_percentage,
        'imputation_method': method,
        'filled_values_count': filled_values_count,
        'remaining_missing_values_count': remaining_missing_values_count,
        'final_records_count': len(df_clean)
    }
    
    return df_clean, missing_values_report

def detect_exchange_rate_outliers(series, iqr_multiplier=1.5):
    first_quartile = series.quantile(0.25)
    third_quartile = series.quantile(0.75)
    interquartile_range = third_quartile - first_quartile
    
    lower_outlier_bound = first_quartile - iqr_multiplier * interquartile_range
    upper_outlier_bound = third_quartile + iqr_multiplier * interquartile_range
    
    outlier_mask = (series < lower_outlier_bound) | (series > upper_outlier_bound)
    
    outlier_statistics = {
        'first_quartile': first_quartile,
        'third_quartile': third_quartile,
        'interquartile_range': interquartile_range,
        'lower_outlier_bound': lower_outlier_bound,
        'upper_outlier_bound': upper_outlier_bound,
        'total_outliers_detected': outlier_mask.sum(),
        'outliers_percentage': (outlier_mask.sum() / len(series)) * 100,
        'outlier_indices': series[outlier_mask].index.tolist()
    }
    
    return outlier_mask, outlier_statistics

def handle_exchange_rate_outliers(df, value_column, method='clip', iqr_multiplier=1.5, window_size=None):
    df_clean = df.copy()
    
    outlier_mask, outlier_statistics = detect_exchange_rate_outliers(df_clean[value_column], iqr_multiplier=iqr_multiplier)
    
    if outlier_statistics['total_outliers_detected'] == 0:
        outlier_report = {
            'total_outliers_detected': 0,
            'outliers_percentage': 0.0,
            'outlier_treatment_method': 'none',
            'outliers_handled_count': 0
        }
        outlier_report.update(outlier_statistics)
        return df_clean, outlier_report
    
    original_exchange_rate_values = df_clean[value_column].copy()
    
    if method == 'clip':
        df_clean.loc[outlier_mask, value_column] = df_clean.loc[outlier_mask, value_column].clip(
            lower=outlier_statistics['lower_outlier_bound'],
            upper=outlier_statistics['upper_outlier_bound']
        )
        outliers_handled_count = outlier_statistics['total_outliers_detected']
    
    elif method == 'remove':
        df_clean = df_clean[~outlier_mask]
        outliers_handled_count = outlier_statistics['total_outliers_detected']
    
    elif method == 'interpolate':
        df_clean.loc[outlier_mask, value_column] = np.nan
        df_clean[value_column] = df_clean[value_column].interpolate(method='linear')
        outliers_handled_count = outlier_statistics['total_outliers_detected']
    
    elif method == 'rolling_median':
        if window_size is None:
            window_size = 3
        
        rolling_median_values = df_clean[value_column].rolling(window=window_size, center=True, min_periods=1).median()
        df_clean.loc[outlier_mask, value_column] = rolling_median_values[outlier_mask]
        outliers_handled_count = outlier_statistics['total_outliers_detected']
    
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤: {method}")
    
    outlier_report = {
        'outlier_treatment_method': method,
        'outliers_handled_count': outliers_handled_count,
        'final_records_count': len(df_clean)
    }
    outlier_report.update(outlier_statistics)
    
    return df_clean, outlier_report

def resample_exchange_rate_data(df, date_column, value_column, target_frequency='D', aggregation_method='mean'):
    df_resampled = df.copy()
    
    df_resampled = df_resampled.set_index(date_column)
    
    original_records_count = len(df_resampled)
    original_data_frequency = pd.infer_freq(df_resampled.index)
    
    numeric_columns = df_resampled.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_columns:
        raise ValueError("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Ä–µ—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    if aggregation_method == 'mean':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).mean()
    elif aggregation_method == 'sum':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).sum()
    elif aggregation_method == 'median':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).median()
    elif aggregation_method == 'min':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).min()
    elif aggregation_method == 'max':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).max()
    elif aggregation_method == 'first':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).first()
    elif aggregation_method == 'last':
        df_resampled = df_resampled[numeric_columns].resample(target_frequency).last()
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {aggregation_method}")
    
    df_resampled = df_resampled.reset_index()
    
    final_records_count = len(df_resampled)
    
    resampling_report = {
        'original_records_count': original_records_count,
        'final_records_count': final_records_count,
        'original_data_frequency': original_data_frequency if original_data_frequency else 'irregular',
        'target_frequency': target_frequency,
        'aggregation_method': aggregation_method,
        'records_count_change': final_records_count - original_records_count
    }
    
    return df_resampled, resampling_report

def preprocess_exchange_rate_pipeline(df, date_column, value_column, preprocessing_config):
    df_processed = df.copy()
    preprocessing_reports = {}
    
    if preprocessing_config.get('standardize_timezone', False):
        df_processed, preprocessing_reports['timezone_standardization'] = standardize_timezone(
            df_processed,
            date_column,
            preprocessing_config.get('target_timezone', 'Europe/Moscow')
        )
    
    if preprocessing_config.get('remove_duplicates', False):
        df_processed, preprocessing_reports['duplicate_removal'] = remove_duplicate_records(
            df_processed,
            date_column,
            preprocessing_config.get('duplicate_strategy', 'first')
        )
    
    if preprocessing_config.get('check_monotonicity', True):
        preprocessing_reports['timestamp_monotonicity'], df_processed = check_timestamp_monotonicity(
            df_processed,
            date_column
        )
    
    if preprocessing_config.get('resample', False):
        df_processed, preprocessing_reports['data_resampling'] = resample_exchange_rate_data(
            df_processed,
            date_column,
            value_column,
            preprocessing_config.get('resample_frequency', 'D'),
            preprocessing_config.get('resample_method', 'mean')
        )
    
    if preprocessing_config.get('handle_missing_values', False):
        df_processed, preprocessing_reports['missing_values_treatment'] = handle_missing_exchange_rates(
            df_processed,
            date_column,
            value_column,
            preprocessing_config.get('missing_values_method', 'linear'),
            preprocessing_config.get('missing_values_window', None)
        )
    
    if preprocessing_config.get('handle_outliers', False):
        df_processed, preprocessing_reports['outlier_treatment'] = handle_exchange_rate_outliers(
            df_processed,
            value_column,
            preprocessing_config.get('outlier_method', 'clip'),
            preprocessing_config.get('iqr_multiplier', 1.5),
            preprocessing_config.get('outlier_window', None)
        )
    
    return df_processed, preprocessing_reports

def calculate_currency_correlations(df, target_currency='JOD=X'):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    """
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±–∫–∏
    numeric_dataframe = df.select_dtypes(include=[np.number])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –≤–∞–ª—é—Ç–∞ –µ—Å—Ç—å –≤ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
    if target_currency not in numeric_dataframe.columns:
        raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –≤–∞–ª—é—Ç–∞ {target_currency} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö")
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    currency_correlations = numeric_dataframe.corr()[target_currency].sort_values(ascending=False)
    
    return currency_correlations

# ========== –û–°–ù–û–í–ù–û–ô –ö–û–î ==========

print("=== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===")
exchange_rate_data = pd.read_csv('Dollar-Exchange.csv')
print(f"–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {exchange_rate_data.shape}")

# –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤–∞–ª—é—Ç–∞–º
print("\n=== –ê–ù–ê–õ–ò–ó –ü–û–õ–ù–û–¢–´ –î–ê–ù–ù–´–• ===")
currency_completeness_analysis = []
for currency_code in exchange_rate_data.columns[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü Date
    missing_values_count = exchange_rate_data[currency_code].isnull().sum()
    missing_percentage = (missing_values_count / len(exchange_rate_data)) * 100
    currency_completeness_analysis.append({
        'currency_code': currency_code,
        'missing_percentage': missing_percentage,
        'valid_records_count': len(exchange_rate_data) - missing_values_count
    })

currency_completeness_df = pd.DataFrame(currency_completeness_analysis).sort_values('missing_percentage')
print("–¢–æ–ø-5 –≤–∞–ª—é—Ç —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤:")
print(currency_completeness_df.head().to_string(index=False))

# === –í–´–ë–û–† –û–ü–¢–ò–ú–ê–õ–¨–ù–´–• –í–ê–õ–Æ–¢ –ù–ê –û–°–ù–û–í–ï –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê ===
print(f"\n=== –í–´–ë–û–† –û–ü–¢–ò–ú–ê–õ–¨–ù–´–• –í–ê–õ–Æ–¢ –ù–ê –û–°–ù–û–í–ï –ö–û–†–†–ï–õ–Ø–¶–ò–ò ===")

target_currency_code = 'JOD=X'  # –ò–æ—Ä–¥–∞–Ω—Å–∫–∏–π –¥–∏–Ω–∞—Ä - —Ü–µ–ª–µ–≤–∞—è –≤–∞–ª—é—Ç–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
try:
    jod_correlations = calculate_currency_correlations(exchange_rate_data, target_currency_code)
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∞–ª—é—Ç (–∏—Å–∫–ª—é—á–∞—è —Å–∞–º—É JOD)
    top_correlated_currencies = jod_correlations[1:6].index.tolist()

    print("üéØ –¢–û–ü-5 –°–ê–ú–´–• –ö–û–†–†–ï–õ–ò–†–û–í–ê–ù–ù–´–• –í–ê–õ–Æ–¢ –° JOD=X:")
    for rank, currency_code in enumerate(top_correlated_currencies, 1):
        correlation_value = jod_correlations[currency_code]
        print(f"   {rank}. {currency_code}: {correlation_value:.3f}")

except Exception as correlation_error:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {correlation_error}")
    print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª—é—Ç—ã —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç...")
    
    # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª—é—Ç—ã —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
    top_currencies_by_completeness = currency_completeness_df.head(6)['currency_code'].tolist()
    # –£–±–∏—Ä–∞–µ–º JOD=X –∏–∑ —Å–ø–∏—Å–∫–∞, –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
    top_currencies_by_completeness = [currency for currency in top_currencies_by_completeness if currency != target_currency_code]
    top_correlated_currencies = top_currencies_by_completeness[:5]
    
    print("üéØ –í–ê–õ–Æ–¢–´ –° –ù–ê–ò–ú–ï–ù–¨–®–ò–ú –ö–û–õ–ò–ß–ï–°–¢–í–û–ú –ü–†–û–ü–£–°–ö–û–í:")
    for rank, currency_code in enumerate(top_correlated_currencies, 1):
        currency_info = currency_completeness_df[currency_completeness_df['currency_code'] == currency_code].iloc[0]
        print(f"   {rank}. {currency_code}: –ø—Ä–æ–ø—É—Å–∫–æ–≤ {currency_info['missing_percentage']:.2f}%")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –≤–∞–ª—é—Ç
print(f"\nüìä –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–• –í–´–ë–†–ê–ù–ù–´–• –í–ê–õ–Æ–¢:")
selected_currency_codes = [target_currency_code] + top_correlated_currencies
for currency_code in selected_currency_codes:
    missing_count = exchange_rate_data[currency_code].isnull().sum()
    missing_percent = (missing_count / len(exchange_rate_data)) * 100
    print(f"   {currency_code}: {missing_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({missing_percent:.2f}%)")

# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
print(f"\n=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê ===")
selected_data_columns = ['Date'] + selected_currency_codes
prepared_dataset = exchange_rate_data[selected_data_columns].copy()

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
column_rename_mapping = {
    'Date': 'timestamp',
    target_currency_code: 'jordanian_dinar_target'  # –ß–µ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
}
# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–Ω—è—Ç–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
feature_currency_mapping = {}
for i, currency_code in enumerate(top_correlated_currencies, 1):
    feature_name = f'feature_currency_{i}_{currency_code.replace("=X", "").replace("/", "_")}'
    column_rename_mapping[currency_code] = feature_name
    feature_currency_mapping[feature_name] = currency_code

prepared_dataset = prepared_dataset.rename(columns=column_rename_mapping)

print(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ –≤–∞–ª—é—Ç—ã:")
print(f"  –¶–µ–ª–µ–≤–∞—è: {target_currency_code} (–ò–æ—Ä–¥–∞–Ω—Å–∫–∏–π –¥–∏–Ω–∞—Ä)")
for i, currency_code in enumerate(top_correlated_currencies, 1):
    feature_name = f'feature_currency_{i}_{currency_code.replace("=X", "").replace("/", "_")}'
    print(f"  –ü—Ä–∏–∑–Ω–∞–∫ {i}: {currency_code} -> {feature_name}")

print(f"\n–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {list(prepared_dataset.columns)}")
print(f"–†–∞–∑–º–µ—Ä: {prepared_dataset.shape}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
print(f"\n=== –ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–°–ö–û–í –ü–ï–†–ï–î –û–ë–†–ê–ë–û–¢–ö–û–ô ===")
numeric_data_columns = ['jordanian_dinar_target'] + [col for col in prepared_dataset.columns if col.startswith('feature_currency_')]
for column in numeric_data_columns:
    missing_count = prepared_dataset[column].isnull().sum()
    print(f"  {column}: {missing_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({missing_count/len(prepared_dataset)*100:.2f}%)")

# === –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú–ò –í–ê–õ–Æ–¢–ê–ú–ò ===
print("\n=== –ó–ê–ü–£–°–ö –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú–ò –í–ê–õ–Æ–¢–ê–ú–ò ===")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
preprocessing_configuration = {
    'standardize_timezone': True,
    'target_timezone': 'Europe/Moscow',
    'remove_duplicates': True,
    'duplicate_strategy': 'first',
    'check_monotonicity': True,
    'resample': False,
    'handle_missing_values': True,
    'missing_values_method': 'forward_fill',
    'missing_values_window': 3,
    'handle_outliers': False,
}

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
processed_dataset, preprocessing_reports = preprocess_exchange_rate_pipeline(
    df=prepared_dataset,
    date_column='timestamp',
    value_column='jordanian_dinar_target',
    preprocessing_config=preprocessing_configuration
)

# –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –í–°–ï –ü–†–û–ü–£–°–ö–ò –í–û –í–°–ï–• –ö–û–õ–û–ù–ö–ê–• –°–†–ê–ó–£
print("\n=== –û–ë–†–ê–ë–û–¢–ö–ê –í–°–ï–• –ü–†–û–ü–£–°–ö–û–í ===")

# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤–æ –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
for column in numeric_data_columns:
    missing_before = processed_dataset[column].isnull().sum()
    if missing_before > 0:
        # –°–Ω–∞—á–∞–ª–∞ forward fill, –ø–æ—Ç–æ–º backward fill –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
        processed_dataset[column] = processed_dataset[column].ffill().bfill()
        missing_after = processed_dataset[column].isnull().sum()
        print(f"  {column}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ {missing_before} –ø—Ä–æ–ø—É—Å–∫–æ–≤")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –ø—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
print(f"\n=== –ü–†–û–í–ï–†–ö–ê –û–¢–°–£–¢–°–¢–í–ò–Ø –ü–†–û–ü–£–°–ö–û–í ===")
total_missing_values = processed_dataset[numeric_data_columns].isnull().sum().sum()
if total_missing_values == 0:
    print("‚úÖ –í—Å–µ –ø—Ä–æ–ø—É—Å–∫–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã!")
else:
    print(f"‚ö†Ô∏è –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–ø—É—Å–∫–æ–≤: {total_missing_values}")

# –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç—ã
print("\n=== –û–¢–ß–ï–¢–´ –û –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ï ===")
for processing_stage, report in preprocessing_reports.items():
    print(f"\n{processing_stage.upper()}:")
    for key, value in report.items():
        if key not in ['outlier_indices']:
            print(f"  {key}: {value}")

# === –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
print("\n=== –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ê–ù–ù–´–• ===")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {len(processed_dataset)}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len([col for col in processed_dataset.columns if col.startswith('feature_currency_')])}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
optimized_dataset_filename = 'preprocessed_exchange_rates_dataset.csv'
processed_dataset.to_csv(optimized_dataset_filename, index=False)

print(f"\n‚úÖ –î–ê–ù–ù–´–ï –£–°–ü–ï–®–ù–û –û–ë–†–ê–ë–û–¢–ê–ù–´ –ò –°–û–•–†–ê–ù–ï–ù–´!")
print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {processed_dataset.shape}")
print(f"üìÖ –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {processed_dataset['timestamp'].min()} - {processed_dataset['timestamp'].max()}")
print(f"üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {optimized_dataset_filename}")

# –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–• –î–ê–ù–ù–´–•:")

print(f"\nüéØ –¶–ï–õ–ï–í–ê–Ø –ü–ï–†–ï–ú–ï–ù–ù–ê–Ø (–ò–æ—Ä–¥–∞–Ω—Å–∫–∏–π –¥–∏–Ω–∞—Ä):")
target_currency_stats = processed_dataset['jordanian_dinar_target'].describe()
print(f"   –°—Ä–µ–¥–Ω–µ–µ: {target_currency_stats['mean']:.4f}")
print(f"   –ú–µ–¥–∏–∞–Ω–∞: {target_currency_stats['50%']:.4f}")
print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {target_currency_stats['std']:.4f}")
print(f"   –ú–∏–Ω–∏–º—É–º: {target_currency_stats['min']:.4f}")
print(f"   –ú–∞–∫—Å–∏–º—É–º: {target_currency_stats['max']:.4f}")
print(f"   –ü—Ä–æ–ø—É—Å–∫–∏: {processed_dataset['jordanian_dinar_target'].isnull().sum()}")

print(f"\nüìä –ü–†–ò–ó–ù–ê–ö–ò (–í–∞–ª—é—Ç—ã-–ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã):")
for feature_column in [col for col in processed_dataset.columns if col.startswith('feature_currency_')]:
    original_currency_code = feature_currency_mapping[feature_column]
    feature_stats = processed_dataset[feature_column].describe()
    print(f"   {feature_column} (–∏—Å—Ö–æ–¥–Ω–æ {original_currency_code}):")
    print(f"      –°—Ä–µ–¥–Ω–µ–µ: {feature_stats['mean']:.4f}")
    print(f"      –ú–µ–¥–∏–∞–Ω–∞: {feature_stats['50%']:.4f}")
    print(f"      –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {feature_stats['std']:.4f}")

print(f"\nüìã –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
print(processed_dataset.head(3))

print(f"\nüîç –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ü–†–û–ü–£–°–ö–û–í:")
all_columns_are_clean = True
for column in processed_dataset.columns:
    missing_count = processed_dataset[column].isnull().sum()
    if missing_count == 0:
        print(f"   {column}: ‚úÖ –Ω–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤")
    else:
        print(f"   {column}: ‚ùå {missing_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤")
        all_columns_are_clean = False

if all_columns_are_clean:
    print(f"\nüéâ –í–°–ï –î–ê–ù–ù–´–ï –ü–û–õ–ù–û–°–¢–¨–Æ –û–ß–ò–©–ï–ù–´ –û–¢ –ü–†–û–ü–£–°–ö–û–í!")
else:
    print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞–ª–∏—Å—å –ø—Ä–æ–ø—É—Å–∫–∏!")

# –†–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
print(f"\nüìà –ö–û–†–†–ï–õ–Ø–¶–ò–ò –í –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –î–ê–ù–ù–´–•:")
try:
    processed_numeric_data = processed_dataset[numeric_data_columns]
    processed_correlation_matrix = processed_numeric_data.corr()['jordanian_dinar_target'].sort_values(ascending=False)
    
    print("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
    for feature_column, correlation_value in processed_correlation_matrix.items():
        if feature_column != 'jordanian_dinar_target':
            original_currency = feature_currency_mapping.get(feature_column, feature_column)
            print(f"   {feature_column} ({original_currency}): {correlation_value:.3f}")
            
except Exception as correlation_error:
    print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏: {correlation_error}")

print(f"\nüí° –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –í–´–ë–†–ê–ù–ù–´–• –í–ê–õ–Æ–¢–ê–•:")
print(f"   - –í—Å–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –≤–∞–ª—é—Ç—ã –∏–º–µ—é—Ç –≤—ã—Å–æ–∫—É—é –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å (>99%)")
print(f"   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ò–æ—Ä–¥–∞–Ω—Å–∫–æ–≥–æ –¥–∏–Ω–∞—Ä–∞")
print(f"   - –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")